---
title: "Homework 4"
author: "Mari Sanders"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readxl)
library(BSDA)
```

# Problem 1 

a) 

$H_0:$ The median blood sugar reading is 120
$H_1:$ The median blood sugar reading is less than 120. 

```{r}
blood_sugar <- c(125,123,117, 123, 115, 112, 128, 118, 124, 111, 116, 
                       109, 125, 120, 113, 123, 112, 118, 121, 118, 122, 115,105, 
                       118, 131)

SIGN.test(blood_sugar, md = 120, alternative = "less")

```

The `p-value` is 0.2706, which means we would fail to reject the null hypothesis that the median blood sugar reading is 120. The `test_statistic` is 10. 

b)

$H_0:$ The median blood sugar readings is 120. 
$H_1:$ The median blood sugar readings is less than 120. 

```{r}
wilcox.test(blood_sugar, mu = 120, alternative = "less")
```

The `p_value` is 0.1447, which means we would fail to reject the null hypothesis that the median blood sugar readings is equal to 120. The `test_statistic` is 112.5. 

# Problem 2 

a) 

```{r}
brain <- read_xlsx("data/Brain.xlsx") %>% janitor::clean_names() 
brain %>% 
  slice(-1) %>% 
  ggplot(aes(x = ln_brain_mass, y = glia_neuron_ratio)) +
  geom_smooth(method = "lm", se = FALSE, color = "black") +
  geom_point(color = "red") +
  geom_point(aes(x = brain$ln_brain_mass[1], 
                 y = brain$glia_neuron_ratio[1])) +
  guides(color = "none") +
  theme_classic()
human_ln_brain_mass <- brain$ln_brain_mass[1]

non_human_model <- lm(glia_neuron_ratio ~ ln_brain_mass, data = brain %>% slice(-1))

broom::tidy(non_human_model)

```

`glia_neuron_ratio` = 0.164 + 0.181*`ln_brain_mass`

b) 

```{r}
predict(non_human_model, newdata = data.frame(ln_brain_mass = human_ln_brain_mass))
```

This means that human `glia_neuron_ratio` is 1.471458. 


c) 

Since we want to look at humans compared to nonhuman primates, the interval for a prediction of a single new observation. This is because our dataset does not have the human brain mass data and we want to see if it can predict the value. 

d) 

```{r}
predict(non_human_model, newdata = data.frame(ln_brain_mass = human_ln_brain_mass), interval = "prediction", level = 0.95)
mean(brain$glia_neuron_ratio)
mean(brain$ln_brain_mass)
```

We are 95% confident that the predicted glia-neuron ratio for humans is between 1.036047 and 1.906869. 

e) 

The position of the human data point might pull the regression line to it and might not accurately predict the rest of the data points. This would be extrapolation. 

# Problem 3 

a) 

```{r}
heart_disease <- read_csv("data/HeartDisease.csv") 
```

This data has `r nrow(heart_disease)` rows and `r ncol(heart_disease)` columns. It includes `r names(heart_disease)` variables. The main outcome is `ERvisits` and the main predictor is `total cost (in dollars)`. The covariates might be `age`, `complications`, `gender`, and `duration`.

```{r}
heart_disease %>% 
  summarize(
    mean_totalcost = mean(totalcost, na.rm = TRUE),
    median_totalcost = median(totalcost, na.rm = TRUE),
    sd_totalcost = sd(totalcost, na.rm = TRUE),
    mean_ervisits = mean(ERvisits, na.rm = TRUE),
    sd_ervisits = sd(ERvisits, na.rm = TRUE),
    mean_age = mean(age, na.rm = TRUE),
    sd_age = sd(age, na.rm = TRUE),
    mean_complications = mean(complications, na.rm = TRUE),
    sd_complications = sd(complications, na.rm = TRUE),
    mean_duration = mean(duration, na.rm = TRUE),
    sd_duration = sd(duration, na.rm = TRUE)
  ) %>% knitr::kable()
```

b) 

```{r}
heart_disease %>% 
  ggplot(aes(x = totalcost)) + geom_histogram()

heart_disease %>% 
  ggplot(aes(x = log(totalcost) + 1)) + geom_histogram()

```

Doing a log transformation seems to make the data look more normal. It originally looked exponential. 

c) 

```{r}
heart_data <- 
  heart_disease %>% 
  drop_na() %>% 
  mutate(comp_bin = ifelse(complications > 0, 1, 0), 
         log_totalcost = log(totalcost + 1))
```

d) 

```{r}
heart_data %>% 
  ggplot(aes(x = ERvisits, y = log_totalcost)) + geom_point() + geom_smooth(method = lm)
model <- lm(log_totalcost~ERvisits, data = heart_data)
broom::tidy(model)
```

`ERvisits` changes by 0.452 as `log_totalcost` increases by one unit. 

`log_totalcost` is significant but the intercept, which is `ERvisits` is not significant because the p-value is greater than 0.05. 

e) 

i) 

Testing for Effect Modifications

```{r}
mlr <- lm(log_totalcost ~ ERvisits*comp_bin , data = heart_data)

broom::tidy(mlr)
```

`comp_bin` is significant, as well as `ERvisits` and the intercept `log_totalcost`, but the interaction between `ERvisits` and `comp_bin` is not significant. 

Since `comp_bin` is not significant, it is not an effect modifier.

ii) 

Testing for Confounding

```{r}
base_model <- lm(log_totalcost ~ ERvisits, data = heart_data)
summary(base_model)
broom::tidy(base_model)

confounder_model <- lm(log_totalcost ~ ERvisits + comp_bin, data = heart_data)
broom::tidy(confounder_model)
```

Since `comp_bin` is significant, with a `p.value` of `1.38e-9`, so this shows that `comp_bin` is statistically significant and associated with `totalcost` even when controlling for `ERvisits`. 

iii) 


f) 

```{r}
extended_model <- lm(log_totalcost ~ ERvisits + comp_bin + age + gender + duration, data = heart_data)
summary(extended_model)
broom::tidy(extended_model)

base_model <- lm(log_totalcost ~ ERvisits, data = heart_data)
summary(base_model)
broom::tidy(base_model)
```

It looks like all the variables, except for `gender` is significant and should be included in the model

```{r}
final_model <- lm(log_totalcost ~ ERvisits + comp_bin + age + duration, data = heart_data)
summary(final_model)
broom::tidy(final_model)
```

Compared to the base_model, the $r^2$ value is higher for the `final_model` that includes multiple parameters. 
